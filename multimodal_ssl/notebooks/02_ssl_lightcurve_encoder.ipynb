{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models.transformer_utils import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectid</th>\n",
       "      <th>filterid</th>\n",
       "      <th>fieldid</th>\n",
       "      <th>rcid</th>\n",
       "      <th>objra</th>\n",
       "      <th>objdec</th>\n",
       "      <th>nepochs</th>\n",
       "      <th>hmjd</th>\n",
       "      <th>mag</th>\n",
       "      <th>magerr</th>\n",
       "      <th>clrcoeff</th>\n",
       "      <th>catflags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>858101100000000</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "      <td>121.166801</td>\n",
       "      <td>74.119148</td>\n",
       "      <td>181</td>\n",
       "      <td>[58247.17837, 58347.49742, 58362.49149, 58368....</td>\n",
       "      <td>[20.790735, 20.31993, 20.606007, 21.471548, 21...</td>\n",
       "      <td>[0.20726836, 0.15888518, 0.18998587, 0.2709626...</td>\n",
       "      <td>[-0.08939283, -0.009556137, -0.006476993, -0.1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>858101100000001</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "      <td>119.970047</td>\n",
       "      <td>74.188736</td>\n",
       "      <td>610</td>\n",
       "      <td>[58207.2526, 58207.2744, 58210.25232, 58229.15...</td>\n",
       "      <td>[16.1671, 16.162312, 16.10719, 16.154564, 16.1...</td>\n",
       "      <td>[0.014392191, 0.014376102, 0.014196091, 0.0143...</td>\n",
       "      <td>[-0.082320735, -0.08643703, -0.048796955, -0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 32768, 0, 0, 0, 0, 32768, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>858101100000002</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "      <td>120.869438</td>\n",
       "      <td>74.136711</td>\n",
       "      <td>610</td>\n",
       "      <td>[58207.25263, 58207.27442, 58210.25234, 58229....</td>\n",
       "      <td>[16.092796, 16.05233, 16.037151, 16.08971, 16....</td>\n",
       "      <td>[0.014150625, 0.014026101, 0.01398062, 0.01414...</td>\n",
       "      <td>[-0.082320735, -0.08643703, -0.048796955, -0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 32768, 0, 0, 0, 0, 32768, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>858101100000003</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "      <td>121.366310</td>\n",
       "      <td>74.105324</td>\n",
       "      <td>598</td>\n",
       "      <td>[58207.25264, 58207.27444, 58210.25236, 58229....</td>\n",
       "      <td>[17.77512, 17.661259, 17.747467, 17.815546, 17...</td>\n",
       "      <td>[0.02695156, 0.02533685, 0.026544018, 0.027565...</td>\n",
       "      <td>[-0.082320735, -0.08643703, -0.048796955, -0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 32768, 0, 0, 0, 0, 32768, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>858101100000005</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "      <td>121.104126</td>\n",
       "      <td>74.121140</td>\n",
       "      <td>379</td>\n",
       "      <td>[58244.2015, 58247.17837, 58255.19753, 58346.4...</td>\n",
       "      <td>[20.145699, 20.32862, 20.048012, 20.142803, 19...</td>\n",
       "      <td>[0.14019924, 0.159865, 0.13053155, 0.13990413,...</td>\n",
       "      <td>[-0.0957851, -0.08939283, -0.0756015, -0.03298...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          objectid  filterid  fieldid  rcid       objra     objdec  nepochs  \\\n",
       "0  858101100000000         1      858     0  121.166801  74.119148      181   \n",
       "1  858101100000001         1      858     0  119.970047  74.188736      610   \n",
       "2  858101100000002         1      858     0  120.869438  74.136711      610   \n",
       "3  858101100000003         1      858     0  121.366310  74.105324      598   \n",
       "5  858101100000005         1      858     0  121.104126  74.121140      379   \n",
       "\n",
       "                                                hmjd  \\\n",
       "0  [58247.17837, 58347.49742, 58362.49149, 58368....   \n",
       "1  [58207.2526, 58207.2744, 58210.25232, 58229.15...   \n",
       "2  [58207.25263, 58207.27442, 58210.25234, 58229....   \n",
       "3  [58207.25264, 58207.27444, 58210.25236, 58229....   \n",
       "5  [58244.2015, 58247.17837, 58255.19753, 58346.4...   \n",
       "\n",
       "                                                 mag  \\\n",
       "0  [20.790735, 20.31993, 20.606007, 21.471548, 21...   \n",
       "1  [16.1671, 16.162312, 16.10719, 16.154564, 16.1...   \n",
       "2  [16.092796, 16.05233, 16.037151, 16.08971, 16....   \n",
       "3  [17.77512, 17.661259, 17.747467, 17.815546, 17...   \n",
       "5  [20.145699, 20.32862, 20.048012, 20.142803, 19...   \n",
       "\n",
       "                                              magerr  \\\n",
       "0  [0.20726836, 0.15888518, 0.18998587, 0.2709626...   \n",
       "1  [0.014392191, 0.014376102, 0.014196091, 0.0143...   \n",
       "2  [0.014150625, 0.014026101, 0.01398062, 0.01414...   \n",
       "3  [0.02695156, 0.02533685, 0.026544018, 0.027565...   \n",
       "5  [0.14019924, 0.159865, 0.13053155, 0.13990413,...   \n",
       "\n",
       "                                            clrcoeff  \\\n",
       "0  [-0.08939283, -0.009556137, -0.006476993, -0.1...   \n",
       "1  [-0.082320735, -0.08643703, -0.048796955, -0.0...   \n",
       "2  [-0.082320735, -0.08643703, -0.048796955, -0.0...   \n",
       "3  [-0.082320735, -0.08643703, -0.048796955, -0.0...   \n",
       "5  [-0.0957851, -0.08939283, -0.0756015, -0.03298...   \n",
       "\n",
       "                                            catflags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 32768, 0, 0, 0, 0, 32768, 0, 0, 0...  \n",
       "2  [0, 0, 0, 0, 32768, 0, 0, 0, 0, 32768, 0, 0, 0...  \n",
       "3  [0, 0, 0, 0, 32768, 0, 0, 0, 0, 32768, 0, 0, 0...  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bulk_df = pq.read_table(\"~/Downloads/ztf_000858_zg_c01_q1_dr19.parquet\").to_pandas()\n",
    "bulk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = bulk_df[bulk_df['mag'].apply(lambda x: len(x) >= 5)]\n",
    "\n",
    "n_max_obs = 100\n",
    "# For entries with > 200 observations, randomly sample 200 observations (hmjd, mag, and magerr with same sample) from the light curve\n",
    "# Pad the entries to 200 observations with zeros and create a mask array\n",
    "mask_list = [] \n",
    "for i in filtered_df.index:\n",
    "    if len(filtered_df.loc[i]['mag']) > n_max_obs:\n",
    "        mask = np.ones(n_max_obs, dtype=bool)\n",
    "        mask_list.append(mask)\n",
    "        indices = np.random.choice(len(filtered_df.loc[i]['mag']), n_max_obs)\n",
    "        filtered_df.at[i, 'hmjd'] = filtered_df.loc[i]['hmjd'][indices]\n",
    "\n",
    "        # Sort the observations by time\n",
    "        sorted_indices = np.argsort(filtered_df.loc[i]['hmjd'])\n",
    "        filtered_df.at[i, 'hmjd'] = filtered_df.loc[i]['hmjd'][sorted_indices]\n",
    "        filtered_df.at[i, 'mag'] = filtered_df.loc[i]['mag'][sorted_indices]\n",
    "        filtered_df.at[i, 'magerr'] = filtered_df.loc[i]['magerr'][sorted_indices]\n",
    "    else:\n",
    "        mask = np.zeros(n_max_obs, dtype=bool)\n",
    "        mask[:len(filtered_df.loc[i]['mag'])] = True\n",
    "        mask_list.append(mask)\n",
    "        \n",
    "        # Pad the arrays with zeros\n",
    "        filtered_df.at[i, 'mag'] = np.pad(filtered_df.loc[i]['mag'], (0, n_max_obs - len(filtered_df.loc[i]['mag'])), 'constant')\n",
    "        filtered_df.at[i, 'magerr'] = np.pad(filtered_df.loc[i]['magerr'], (0, n_max_obs - len(filtered_df.loc[i]['magerr'])), 'constant')\n",
    "        filtered_df.at[i, 'hmjd'] = np.pad(filtered_df.loc[i]['hmjd'], (0, n_max_obs - len(filtered_df.loc[i]['hmjd'])), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7345"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TimePositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_emb):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_emb = d_emb\n",
    "\n",
    "    def forward(self, t):\n",
    "        pe = torch.zeros(t.shape[0], t.shape[1], self.d_emb).to(t.device)  # (B, T, D)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_emb, 2).float() * (-math.log(10000.0) / self.d_emb))[None, None, :].to(t.device)  # (1, 1, D / 2)\n",
    "        t = t.unsqueeze(2)  # (B, 1, T)\n",
    "        pe[:, :, 0::2] = torch.sin(t * div_term)  # (B, T, D / 2)\n",
    "        pe[:, :, 1::2] = torch.cos(t * div_term)  # (B, T, D / 2)\n",
    "        return pe  # (B, T, D)\n",
    "\n",
    "class TransformerWithTimeEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for classifying sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_out=1, **kwargs):\n",
    "        \"\"\"\n",
    "        :param emb: Embedding dimension\n",
    "        :param heads: nr. of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param seq_length: Expected maximum sequence length\n",
    "        :param num_classes: Number of classes.\n",
    "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_mag = nn.Linear(in_features=1, out_features=kwargs['emb'])\n",
    "        self.embedding_t = TimePositionalEncoding(kwargs['emb'])\n",
    "        self.transformer = Transformer(**kwargs)\n",
    "        self.projection = nn.Linear(in_features=kwargs['emb'], out_features=n_out)\n",
    "\n",
    "    def forward(self, x, t, mask=None):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        t = t - t[:, 0].unsqueeze(1)\n",
    "        t_emb = self.embedding_t(t)\n",
    "        x = self.embedding_mag(x) + t_emb\n",
    "        x = self.transformer(x, mask)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "transformer = TransformerWithTimeEmbeddings(n_out=1, emb=128, heads=2, depth=2)\n",
    "pe = TimePositionalEncoding(d_emb=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <7702F607-92FA-3D67-9D09-0710D936B85A> /opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <1E9FA061-EA31-3736-81D0-79A33B965097> /opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLightCurveEncoder(pl.LightningModule):\n",
    "    def __init__(self, f_mask=0.2, optimizer_kwargs={\"weight_decay\": 1e-4}, lr=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.lr = lr\n",
    "        self.f_mask = f_mask\n",
    "\n",
    "        self.net = TransformerWithTimeEmbeddings(n_out=1, emb=128, heads=2, depth=4)\n",
    "\n",
    "    def forward(self, x, t, mask=None):\n",
    "        x = x[..., None]\n",
    "        x = self.net(x, t, mask)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, **self.optimizer_kwargs)\n",
    "        return {\"optimizer\": optimizer}\n",
    "\n",
    "    def masked_pred(self, x, t, padding_mask, f_mask=0.15):\n",
    "        # Get number of observations using padding mask\n",
    "        n_obs = padding_mask.sum(dim=1)\n",
    "\n",
    "        n_obs_to_keep = torch.round((1 - f_mask) * n_obs).int()\n",
    "        mask = torch.zeros_like(x).bool()  # Default to full mask\n",
    "        mask_pred = torch.zeros_like(x).bool()\n",
    "        \n",
    "        # Randomly keep a contiguous set of observations\n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            start = torch.randint(0, n_obs[i] - n_obs_to_keep[i] + 1, (1,)).item()\n",
    "            end = start + n_obs_to_keep[i]\n",
    "            \n",
    "            mask[i, start:end] = True\n",
    "\n",
    "            mask_pred[i, end:n_obs[i]] = True\n",
    "            mask_pred[i, :start] = True\n",
    "        \n",
    "        x_pred = self(x, t, mask)[..., 0]\n",
    "        return x[mask_pred], x_pred[mask_pred]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, t, padding_mask = batch\n",
    "        x, x_pred = self.masked_pred(x, t, padding_mask, f_mask=self.f_mask)\n",
    "        loss = nn.MSELoss()(x, x_pred)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, t, padding_mask = batch\n",
    "        x, x_pred = self.masked_pred(x, t, padding_mask, f_mask=self.f_mask)\n",
    "        loss = nn.MSELoss()(x, x_pred)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag = filtered_df['mag'].array\n",
    "magerr = filtered_df['magerr'].array\n",
    "hmjd = filtered_df['hmjd'].array\n",
    "\n",
    "# Convert to torch tensors\n",
    "mag = torch.Tensor(mag)\n",
    "magerr = torch.Tensor(magerr)\n",
    "hmjd = torch.Tensor(hmjd)\n",
    "mask = torch.Tensor(mask_list).to(torch.bool)\n",
    "\n",
    "# Standardize mag and magerr\n",
    "mag_mean = mag.mean()\n",
    "mag_std = mag.std()\n",
    "mag = (mag - mag_mean) / mag_std\n",
    "magerr = magerr / mag_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlce = MaskedLightCurveEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mag_dummy = mag[:1]\n",
    "# print((mlce(mag_dummy, hmjd[:1], mask[:1])[..., 0] * mask[:1]).sum())\n",
    "\n",
    "# mag_dummy[mag_dummy == 0] = 12.\n",
    "# print((mlce(mag_dummy, hmjd[:1], mask[:1])[..., 0] * mask[:1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "val_fraction = 0.\n",
    "batch_size = 64\n",
    "n_samples_val = int(val_fraction * mag.shape[0])\n",
    "\n",
    "dataset = TensorDataset(mag, hmjd, mask)\n",
    "\n",
    "dataset_train, dataset_val = random_split(dataset, [mag.shape[0] - n_samples_val, n_samples_val])\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name | Type                          | Params\n",
      "-------------------------------------------------------\n",
      "0 | net  | TransformerWithTimeEmbeddings | 1.1 M \n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.220     Total estimated model params size (MB)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1558: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86001d037e3a4e4e8e844594e887549f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=30, accelerator='cpu')\n",
    "trainer.fit(model=mlce, train_dataloaders=train_loader,);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ii \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m33\u001b[39m\n\u001b[0;32m----> 5\u001b[0m mask_tofill \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m[ii:ii \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# mask_tofill[:, 0:0] = False\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(hmjd[ii], mlce(mag[ii:ii \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], hmjd[ii:ii \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], mask_tofill)[\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ii = 33\n",
    "\n",
    "mask_tofill = mask[ii:ii + 1].clone()\n",
    "\n",
    "# mask_tofill[:, 0:0] = False\n",
    "\n",
    "plt.plot(hmjd[ii], mlce(mag[ii:ii + 1], hmjd[ii:ii + 1], mask_tofill)[0, :, 0].detach().numpy())\n",
    "plt.plot(hmjd[ii], mag[ii:ii + 1].detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
