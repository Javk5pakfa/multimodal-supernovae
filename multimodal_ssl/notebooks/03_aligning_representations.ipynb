{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised and multi-modal representation Learning: Notebook 3\n",
    "\n",
    "Here we will align the image and light curve representations with contrastive learning. Optionally, we can use the light curve encoder we trained previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-modal contrastive learning with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light curve encoding via masked self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load host images and inspect shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_host_imgs = \"../data/ZTFBTS/hostImgs/\"\n",
    "host_imgs = []\n",
    "\n",
    "for filename in os.listdir(dir_host_imgs):\n",
    "    file_path = os.path.join(dir_host_imgs, filename)\n",
    "    if file_path.endswith(\".png\"):\n",
    "        host_img = Image.open(file_path).convert('RGB')\n",
    "        host_img = np.asarray(host_img)\n",
    "        host_imgs.append(host_img)\n",
    "\n",
    "host_imgs = np.array(host_imgs)\n",
    "\n",
    "host_imgs = torch.from_numpy(host_imgs).float()\n",
    "host_imgs = rearrange(host_imgs, 'b h w c -> b c h w')\n",
    "\n",
    "# Normalize\n",
    "host_imgs /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load light curves and pre-process them just like in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>mag</th>\n",
       "      <th>magerr</th>\n",
       "      <th>band</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58312.219097</td>\n",
       "      <td>20.132299</td>\n",
       "      <td>0.252360</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58319.205984</td>\n",
       "      <td>18.713728</td>\n",
       "      <td>0.104188</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58319.224942</td>\n",
       "      <td>18.808235</td>\n",
       "      <td>0.092660</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58320.174525</td>\n",
       "      <td>18.467438</td>\n",
       "      <td>0.093920</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58324.179444</td>\n",
       "      <td>18.514769</td>\n",
       "      <td>0.117073</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           time        mag    magerr band\n",
       "0  58312.219097  20.132299  0.252360    R\n",
       "1  58319.205984  18.713728  0.104188    g\n",
       "2  58319.224942  18.808235  0.092660    R\n",
       "3  58320.174525  18.467438  0.093920    g\n",
       "4  58324.179444  18.514769  0.117073    R"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_light_curves = \"../data/ZTFBTS/light-curves/\"\n",
    "\n",
    "def open_light_curve_csv(filename):\n",
    "    file_path = os.path.join(dir_light_curves, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "light_curve_df = open_light_curve_csv(\"ZTF18aailmnv.csv\")\n",
    "light_curve_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5170/5170 [00:03<00:00, 1579.60it/s]\n"
     ]
    }
   ],
   "source": [
    "band = 'R'\n",
    "n_max_obs = 50\n",
    "\n",
    "lightcurve_files = os.listdir(dir_light_curves)\n",
    "\n",
    "# For entries with > n_max_obs observations, randomly sample n_max_obs observations (hmjd, mag, and magerr with same sample) from the light curve\n",
    "# Pad the entries to n_max_obs observations with zeros and create a mask array\n",
    "mask_list = []\n",
    "mag_list = []\n",
    "magerr_list = []\n",
    "time_list = []\n",
    "\n",
    "for filename in tqdm(lightcurve_files):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        light_curve_df = open_light_curve_csv(filename)\n",
    "        \n",
    "        # Make sure the csv contains 'time', 'mag', 'magerr', and 'band' columns\n",
    "        if not all(col in light_curve_df.columns for col in ['time', 'mag', 'magerr', 'band']):\n",
    "            continue\n",
    "        \n",
    "        bands = light_curve_df['band'].unique()\n",
    "        df_band = light_curve_df[light_curve_df['band'] == band]\n",
    "\n",
    "        if len(df_band['mag']) > n_max_obs:\n",
    "            mask = np.ones(n_max_obs, dtype=bool)\n",
    "            mask_list.append(mask)\n",
    "            indices = np.random.choice(len(df_band['mag']), n_max_obs)\n",
    "            time = df_band['time'].values[indices]\n",
    "            mag = df_band['mag'].values[indices]\n",
    "            magerr = df_band['magerr'].values[indices]\n",
    "        else:\n",
    "            mask = np.zeros(n_max_obs, dtype=bool)\n",
    "            mask[:len(df_band['mag'])] = True\n",
    "            mask_list.append(mask)\n",
    "\n",
    "            # Pad the arrays with zeros\n",
    "            time = np.pad(df_band['mag'], (0, n_max_obs - len(df_band['mag'])), 'constant')\n",
    "            mag = np.pad(df_band['magerr'], (0, n_max_obs - len(df_band['magerr'])), 'constant')\n",
    "            magerr = np.pad(df_band['time'], (0, n_max_obs - len(df_band['time'])), 'constant')\n",
    "            \n",
    "        time_list.append(time)\n",
    "        mag_list.append(mag)\n",
    "        magerr_list.append(magerr)\n",
    "\n",
    "time_ary = np.array(time_list)\n",
    "mag_ary = np.array(mag_list)\n",
    "magerr_ary = np.array(magerr_list)\n",
    "mask_ary = np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5170, 50), (5170, 50), (5170, 50), (5170, 50))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect shapes\n",
    "time_ary.shape, mag_ary.shape, magerr_ary.shape, mask_ary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that adds the input to the output of a function.\n",
    "    \"\"\"\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the function and add the input to the result\n",
    "        return self.fn(x) + x\n",
    "    \n",
    "\n",
    "class ConvMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvMixer model, a simple and efficient convolutional neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, depth, channels=1, kernel_size=5, patch_size=8, n_out=128):\n",
    "        super(ConvMixer, self).__init__()\n",
    "\n",
    "        # Initial convolution layer\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(channels, dim, kernel_size=patch_size, stride=patch_size, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(dim),\n",
    "        )\n",
    "\n",
    "        # Adding depth number of ConvMixer layers\n",
    "        for _ in range(depth):\n",
    "            self.net.append(nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "            ))\n",
    "\n",
    "        # Projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, n_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.net(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convmixer = ConvMixer(dim=60, depth=8, channels=3, kernel_size=5, patch_size=8, n_out=128)\n",
    "convmixer(host_imgs[:4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from models.transformer_utils import Transformer\n",
    "\n",
    "class TimePositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_emb):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_emb = d_emb\n",
    "\n",
    "    def forward(self, t):\n",
    "        pe = torch.zeros(t.shape[0], t.shape[1], self.d_emb).to(t.device)  # (B, T, D)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_emb, 2).float() * (-math.log(10000.0) / self.d_emb))[None, None, :].to(t.device)  # (1, 1, D / 2)\n",
    "        t = t.unsqueeze(2)  # (B, 1, T)\n",
    "        pe[:, :, 0::2] = torch.sin(t * div_term)  # (B, T, D / 2)\n",
    "        pe[:, :, 1::2] = torch.cos(t * div_term)  # (B, T, D / 2)\n",
    "        return pe  # (B, T, D)\n",
    "\n",
    "class TransformerWithTimeEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for classifying sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_out, **kwargs):\n",
    "        \"\"\"\n",
    "        :param emb: Embedding dimension\n",
    "        :param heads: nr. of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param seq_length: Expected maximum sequence length\n",
    "        :param num_classes: Number of classes.\n",
    "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_mag = nn.Linear(in_features=1, out_features=kwargs['emb'])\n",
    "        self.embedding_t = TimePositionalEncoding(kwargs['emb'])\n",
    "        self.transformer = Transformer(**kwargs)\n",
    "\n",
    "        self.projection = nn.Linear(kwargs['emb'], n_out)\n",
    "\n",
    "    def forward(self, x, t, mask=None):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        t = t - t[:, 0].unsqueeze(1)\n",
    "        t_emb = self.embedding_t(t)\n",
    "        x = self.embedding_mag(x) + t_emb\n",
    "        x = self.transformer(x, mask)  # (B, T, D)\n",
    "        \n",
    "        # Zero out the masked values\n",
    "        x = x * mask[:, :, None]\n",
    "\n",
    "        # Max pool\n",
    "        x = x.max(dim=1)[0]\n",
    "        \n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "transformer = TransformerWithTimeEmbeddings(n_out=128, emb=128, heads=1, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time and mag tensors\n",
    "time = torch.from_numpy(time_ary).float()\n",
    "mag = torch.from_numpy(mag_ary).float()\n",
    "mask = torch.from_numpy(mask_ary).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass a batch through\n",
    "transformer(mag[:4][..., None], time[:4], mask[:4]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive-style losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirection (symmetric between modalities) InfoNCE loss to compute alignment between image and light curve representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <7702F607-92FA-3D67-9D09-0710D936B85A> /opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <1E9FA061-EA31-3736-81D0-79A33B965097> /opt/homebrew/Caskroom/miniforge/base/envs/cfm/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "val_fraction = 0.1\n",
    "batch_size = 64\n",
    "n_samples_val = int(val_fraction * mag.shape[0])\n",
    "\n",
    "dataset = TensorDataset(host_imgs, mag, time, mask)\n",
    "\n",
    "dataset_train, dataset_val = random_split(dataset, [mag.shape[0] - n_samples_val, n_samples_val])\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_loss(image_embeddings, text_embeddings, temperature=1):\n",
    "    \n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    logits = (text_embeddings @ image_embeddings.T) / temperature\n",
    "    images_similarity = image_embeddings @ image_embeddings.T\n",
    "    texts_similarity = text_embeddings @ text_embeddings.T\n",
    "    targets = F.softmax((images_similarity + texts_similarity) / 2 * temperature, dim=-1)\n",
    "    images_loss = (-targets.T * log_softmax(logits.T)).sum(1)\n",
    "    texts_loss = (-targets * log_softmax(logits)).sum(1)\n",
    "    return (images_loss + texts_loss) / 2.0\n",
    "\n",
    "def sigmoid_loss(image_embeds, text_embeds, logit_scale=1., logit_bias=2.73):\n",
    "\n",
    "    bs = text_embeds.shape[0]\n",
    "    \n",
    "    labels = 2 * torch.eye(bs) - torch.ones((bs, bs))\n",
    "    labels = labels.to(text_embeds.device)\n",
    "\n",
    "    logits = text_embeds @ image_embeds.t() * logit_scale + logit_bias\n",
    "    logits = logits.to(torch.float64)\n",
    "    \n",
    "    positive_loss = -torch.mean(torch.log(torch.sigmoid(labels * logits)))\n",
    "    \n",
    "    shifted_image_embeds = torch.roll(image_embeds, 1, dims=0)\n",
    "    negative_logits = text_embeds @ shifted_image_embeds.t() * logit_scale + logit_bias    \n",
    "    negative_loss = -torch.mean(torch.log(1 - torch.sigmoid(negative_logits)))\n",
    "    \n",
    "    loss = positive_loss + negative_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightCurveImageCLIP(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 enc_dim=64,\n",
    "                 temperature=10.,\n",
    "                 transformer_kwargs={\"n_out\":128, \"emb\":128, \"heads\":2, \"depth\":2}, \n",
    "                 conv_kwargs = {'dim': 60, 'depth': 4, 'channels': 3, 'kernel_size': 5, 'patch_size': 8, 'n_out': 128}, \n",
    "                 optimizer_kwargs={}, lr=1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.enc_dim = enc_dim\n",
    "\n",
    "        # Make temperature a learnable parameter\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(temperature)), requires_grad=True)\n",
    "        self.logit_bias = nn.Parameter(torch.tensor(-10.), requires_grad=True)\n",
    "\n",
    "        self.lightcurve_encoder = TransformerWithTimeEmbeddings(**transformer_kwargs)\n",
    "        self.image_encoder = ConvMixer(**conv_kwargs)\n",
    "\n",
    "        self.lightcurve_projection = nn.Linear(transformer_kwargs['n_out'], enc_dim)\n",
    "        self.image_projection = nn.Linear(conv_kwargs['n_out'], enc_dim)\n",
    "\n",
    "    def forward(self, x_img, x_lc, t_lc, mask_lc=None):\n",
    "        \n",
    "        # Light curve encoder\n",
    "        x_lc = self.lightcurve_embeddings_with_projection(x_lc, t_lc, mask_lc)\n",
    "        \n",
    "        # Image encoder\n",
    "        x_img = self.image_embeddings_with_projection(x_img)\n",
    "    \n",
    "        # Normalized embeddings\n",
    "        return x_img, x_lc\n",
    "    \n",
    "    def image_embeddings_with_projection(self, x_img):\n",
    "        x_img = self.image_encoder(x_img)\n",
    "        x_img = self.image_projection(x_img)\n",
    "        return x_img / x_img.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    def lightcurve_embeddings_with_projection(self, x_lc, t_lc, mask_lc=None):\n",
    "        x_lc = x_lc[..., None]\n",
    "        x_lc = self.lightcurve_encoder(x_lc, t_lc, mask_lc)\n",
    "        x_lc = self.lightcurve_projection(x_lc)\n",
    "        return x_lc / x_lc.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, **self.optimizer_kwargs)\n",
    "        return {\"optimizer\": optimizer}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_img, x_lc, t_lc, mask_lc = batch\n",
    "        x_img, x_lc = self(x_img, x_lc, t_lc, mask_lc)\n",
    "        loss = sigmoid_loss(x_img, x_lc, self.logit_scale, self.logit_bias).mean()\n",
    "        # loss = clip_loss(x_img, x_lc, self.logit_scale,).mean()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_img, x_lc, t_lc, mask_lc = batch\n",
    "        x_img, x_lc = self(x_img, x_lc, t_lc, mask_lc)\n",
    "        loss = sigmoid_loss(x_img, x_lc, self.logit_scale, self.logit_bias).mean()\n",
    "        # loss = clip_loss(x_img, x_lc, self.logit_scale, ).mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1890, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model = LightCurveImageCLIP(temperature=10., lr=1e-4)\n",
    "\n",
    "x_img, x_lc, t_lc, mask_lc = next(iter(train_loader))\n",
    "x_img, x_lc = clip_model(x_img, x_lc, t_lc, mask_lc)\n",
    "\n",
    "sigmoid_loss(x_img, x_lc, clip_model.logit_scale).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                  | Type                          | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | lightcurve_encoder    | TransformerWithTimeEmbeddings | 544 K \n",
      "1 | image_encoder         | ConvMixer                     | 227 K \n",
      "2 | lightcurve_projection | Linear                        | 8.3 K \n",
      "3 | image_projection      | Linear                        | 8.3 K \n",
      "------------------------------------------------------------------------\n",
      "787 K     Trainable params\n",
      "0         Non-trainable params\n",
      "787 K     Total params\n",
      "3.151     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b366d46fc5234a748adb46c5bce4937e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a10ead3de494095856ed74b86835a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca90ca89cbe5492d8b8d6ef993aee441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce2949d93f34e91bf51e88c310189af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f420735cc8214a41885f4f430d591a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11618aab3caa4230960ba944c4c8d0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator='cpu')\n",
    "trainer.fit(model=clip_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
